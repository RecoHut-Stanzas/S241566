{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Debiased Explainable Pairwise Ranking from Implicit Feedback\n",
        "\n",
        "## Executive summary\n",
        "\n",
        "| | |\n",
        "| --- | --- |\n",
        "| Problem | BPR is black-box model and vulnerable to Missing-not-at-random (MNaR) based exposure bias. |\n",
        "| Solution | An explainable loss function and a corresponding Matrix Factorization-based model called Explainable Bayesian Personalized Ranking (EBPR) that generates recommendations along with item-based explanations. |\n",
        "| Dataset | ML-100k, ML-1m, LastFM-200k, Yahoo R3. |\n",
        "| Preprocessing | Interactions were converted into binary interactions, regardless of their values. Then we filtered out users with less than 10 interactions to ensure enough training and evaluation samples for every user and reduce the data sparsity. We follow the standard Leave-One-Out (LOO) procedure that consists of considering the latest interaction of each user as a test item and comparing it to 100 randomly sampled negative items. In the training, we sample, at every epoch, one negative item for every positive user-item interaction. |\n",
        "| Metrics | NDCG@K, HR@K, Mean Explainability Precision (MEP@K), WMEP@K, Avg_Pop@K, EFD@K, and Div@K. |\n",
        "| Hyperparams | NUM_CONFIGURATIONS, NUM_REPS, NUM_EPOCH, WEIGHT_DECAY, NEIGHBORHOOD, TOP_K, LR, OPTIMIZER, SGD_MOMENTUM, RMSPROP_ALPHA, RMSPROP_MOMENTUM, LOO_EVAL, TEST_RATE, USE_CUDA, DEVICE_ID, SAVE_MODELS, SAVE_RESULTS, INT_PER_ITEM. |\n",
        "| Models | BPR, UBPR, EBPR, pUEBPR, UEBPR. |\n",
        "| Cluster | Python 3.7+, PyTorch |\n",
        "| Tags | `Fairness`, `Explainability`, `ExposureBias`, `ExplainableBPR` |\n",
        "| Credits | Khalil Damak |\n",
        "\n",
        "## Issues of BPR-based recommendation models\n",
        "\n",
        "BPR rely on the assumption that implicit feedback data is Missing Completely At Random (MCAR), meaning that the items are equally likely to be observed by the users [40], consequently any missing interaction is missing because the user chose not to interact with it. However, given the abundance of items on most e-commerce, entertainment, and other online platforms, it is safe to assume the impossibility of any user being exposed to all the items. Thus, missing interactions should be considered Missing Not At Random (MNAR). This means that the user may have been exposed to part of the items, but chose not to interact with them, which can be a sign of irrelevance; and was not exposed to the rest of the items. This MNAR property is translated into an exposure bias. This type of bias is usually characterized by a bias against less popular items that have a lower propensity of being observed.\n",
        "\n",
        "Moreover, most accurate recommender systems tend to be black boxes that do not justify why or how an item was recommended to a user. This might engender unfairness issues if, for example, particularly inappropriate or offensive content gets recommended to a user. This kind of unfairness can be better diagnosed and mitigated with an explanation. In fact, it could be important for the user to know why or how the inappropriate item was recommended. For example, an Italian user might think that the movie recommendation “The Godfather\" is offensive because of the way it depicts, in an unfair stereotypical way, a certain Italian community in the US. However, the explanation “Because you liked the movie “Scarface\"\" can be important in this case, because it clarifies that the movie recommendation was not tied to a community, but rather resulted from the user also liking another similar “mafia\" sub-genre movie. Furthermore, explanations have been shown to help users make more accurate decisions, which translates into an increased user satisfaction. Bayesian Personalized Ranking treats comparisons between any positive and negative items the same, regardless of which ones can or cannot be explained. Thus, while BPR aptly captures and models ranking based preference, it does not yet capture an explainable preference.\n",
        "\n",
        "## Explainability Matrix\n",
        "\n",
        "We only have collaborative signals in our case, so we have to rely on these signals only to derive explainability. We can categorize the explanations as user-based or item-based. User-based explanations are based on user similarities and generate explanations in the form of “this item was recommended because certain similar users liked it\". Item-based explanations use item similarities and generate explanations in the form “the item was recommended because you liked similar items\". Both item-based and user-based measures of explainability can be defined by relying solely on the interaction matrix (or rating matrix, depending on the type of feedback). However, in this work, we focus only on item-based explanations which are expected to be more intuitive and informative to the user than user-based explanations. This is because the user knows the items that they interacted with but does not necessarily know their neighbors who have similar interactions with items. That said, a user-based explainability matrix can be similarly defined by applying the same strategy, described below, on the transpose of the interaction matrix. We define the measure of explainability $𝐸_{𝑢𝑖}$ as the probability of user 𝑢 interacting with item 𝑖’s neighbors:\n",
        "\n",
        "$$𝐸_{𝑢𝑖} = 𝑃(𝑌_{𝑢𝑗} = 1|𝑗\\in𝑁_i^\\eta)$$\n",
        "\n",
        "where,\n",
        "\n",
        "- $N_i^\\eta$ is the neighborhood of item 𝑖 which is a set of item 𝑖’s $\\eta$ most similar items given a similarity measure, we can use cosine similarity between items to generate neighborhoods.\n",
        "- $Y_{ui}$ is a Bernoulli random variable that takes value 1 if user 𝑢 interacted with item 𝑖 and 0 otherwise.\n",
        "\n",
        "## Tutorials\n",
        "\n",
        "### Training Explainable BPR model on LastFM dataset\n",
        "\n",
        "[direct link to notebook →](https://github.com/RecoHut-Stanzas/S241566/blob/main/nbs/P394476_Training_Explainable_BPR_model_on_LastFM_dataset.ipynb)\n",
        "\n",
        "![https://github.com/RecoHut-Stanzas/S241566/raw/main/images/process_flow.svg](https://github.com/RecoHut-Stanzas/S241566/raw/main/images/process_flow.svg)\n",
        "\n",
        "## References\n",
        "\n",
        "1. [https://github.com/RecoHut-Stanzas/S241566](https://github.com/RecoHut-Stanzas/S241566)\n",
        "2. [https://arxiv.org/pdf/2107.14768v1.pdf](https://arxiv.org/pdf/2107.14768v1.pdf)\n",
        "3. [https://github.com/KhalilDMK/EBPR](https://github.com/KhalilDMK/EBPR)"
      ],
      "metadata": {
        "id": "lIYdn1woOS1n"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "scratchpad",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}